{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pretty printing has been turned OFF\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Environment import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "状态数： 16\n动作数： 4\n状态转移概率数组长度 16\n{0: [(1.0, 0, -1.0, False)], 1: [(1.0, 1, -1.0, False)], 2: [(1.0, 4, -1.0, False)], 3: [(1.0, 0, -1.0, False)]}\n"
    }
   ],
   "source": [
    "print(\"状态数：\",env.nS)\n",
    "print(\"动作数：\",env.nA)\n",
    "print(\"状态转移概率数组长度\",len(env.P))\n",
    "print(env.P[0]) # {action: [(prob, next_state, reward, isDone)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "init policy: [[0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]]\n"
    }
   ],
   "source": [
    "init_policy = np.ones((env.nS, env.nA), dtype=np.float32) / env.nA\n",
    "print(\"init policy:\", init_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eavl(policy, environment, discount = 1.0 , theta = 0.1):\n",
    "    env = environment\n",
    "    \"\"\"\n",
    "        input: policy (state_num, action_num), enviroment, discount, theta\n",
    "        return: value_function (state_num)\n",
    "    \"\"\"\n",
    "    \n",
    "    V = np.zeros(env.nS, dtype=np.float32) # 价值函数初始化 (16)\n",
    "    \"1.遍历所有状态\"\n",
    "    max_epoch = 50\n",
    "    for epoch in range(max_epoch):\n",
    "        delta = 0. # delta用于每个epoch时进行比较\n",
    "        for state in range(env.nS):\n",
    "            v = 0\n",
    "            \"2.遍历该策略下可以采取的动作\"\n",
    "            for action, action_prob in enumerate(policy[state]):\n",
    "                \"3.遍历在该(策略、动作)可到达的下一个状态\"\n",
    "                for state_prob, next_state, reward, _ in env.P[state][action]:\n",
    "                    v += action_prob * state_prob * (reward + discount * V[next_state])\n",
    "\n",
    "            delta = np.maximum(delta, np.abs(V[state] - v))\n",
    "            V[state] = v\n",
    "            \n",
    "        \"打印调试信息\"\n",
    "        if(epoch % 10 == 0):\n",
    "            print(\"epoch: %d, delta: %.2f.\" % (epoch, delta))\n",
    "        if(delta <= theta):\n",
    "            break\n",
    "    print(\"Final epoch:\", epoch)\n",
    "    return V # (16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0, delta: 1.86.\nepoch: 10, delta: 1.04.\nepoch: 20, delta: 1.00.\nepoch: 30, delta: 1.00.\nepoch: 40, delta: 1.00.\nFinal epoch: 49\nV_pi: [[-50.701645 -50.834114 -50.768112 -50.89453 ]\n [-50.51342  -50.       -49.50351  -50.      ]\n [-49.787567 -47.72631  -45.103867 -50.      ]\n [-50.       -43.69593  -30.746363   0.      ]]\n"
    }
   ],
   "source": [
    "V_pi = policy_eavl(init_policy, env)\n",
    "print(\"V_pi:\",V_pi.reshape(env.desc.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improve(V_pi, environment, discount = 1.):\n",
    "    env = environment\n",
    "    \"\"\"\n",
    "        input: V_pi, enviroment\n",
    "        return: improved_policy\n",
    "    \"\"\"\n",
    "    improved_policy = np.zeros((env.nS, env.nA), dtype=np.float32)\n",
    "    \"1.对每个状态进行遍历\"\n",
    "    for state in range(env.nS):\n",
    "        action_values = np.zeros(env.nA, dtype=np.float32)\n",
    "        \"2.对该状态下可能执行的动作进行遍历\"\n",
    "        for action in range(env.nA):    \n",
    "            \"3.对（状态，动作），遍历所有的下一个转态\"\n",
    "            for state_prob, next_state, reward, done in env.P[state][action]:\n",
    "                if(done and next_state!=15): # 陷入到陷阱而非终点\n",
    "                    action_values[action] = -np.inf\n",
    "                    break\n",
    "                else:\n",
    "                    action_values[action] += state_prob * (reward + discount * V_pi[next_state])\n",
    "        optim_action = np.argmax(action_values)\n",
    "        # print(\"State: %d, action values: \" % (state), action_values)\n",
    "        improved_policy[state][optim_action] = 1. # 设置最优策略中optim_action索引为1，其他action为0\n",
    "    \n",
    "    return improved_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0. 0. 1. 0.]\n [0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [1. 0. 0. 0.]\n [0. 0. 1. 0.]\n [1. 0. 0. 0.]\n [0. 0. 1. 0.]\n [1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 1. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 1. 0. 0.]\n [1. 0. 0. 0.]]\n[[2 3 2 0]\n [2 0 2 0]\n [1 2 2 0]\n [0 1 1 0]]\n"
    }
   ],
   "source": [
    "improved_policy = policy_improve(V_pi, env)\n",
    "print(improved_policy)\n",
    "print(np.argmax(improved_policy, axis=1).reshape(env.desc.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0, delta: 1.86.\nepoch: 10, delta: 1.04.\nepoch: 20, delta: 1.00.\nepoch: 30, delta: 1.00.\nepoch: 40, delta: 1.00.\nFinal epoch: 49\nState: 0, action values:  [-51.701645 -51.834114 -51.51342  -51.701645]\nState: 1, action values:  [-51.834114 -51.768112       -inf -51.701645]\nState: 2, action values:  [-51.768112 -51.89453  -50.50351  -51.834114]\nState: 3, action values:  [-51.89453  -51.89453        -inf -51.768112]\nState: 4, action values:  [-51.701645       -inf -50.787567 -51.51342 ]\nState: 5, action values:  [-inf -inf -inf -inf]\nState: 6, action values:  [-51.768112       -inf -46.103867       -inf]\nState: 7, action values:  [-inf -inf -inf -inf]\nState: 8, action values:  [-51.51342  -48.72631        -inf -50.787567]\nState: 9, action values:  [      -inf -46.103867 -44.69593  -50.787567]\nState: 10, action values:  [-50.50351        -inf -31.746363 -48.72631 ]\nState: 11, action values:  [-inf -inf -inf -inf]\nState: 12, action values:  [-inf -inf -inf -inf]\nState: 13, action values:  [-48.72631  -31.746363 -44.69593        -inf]\nState: 14, action values:  [-46.103867  -1.       -31.746363 -44.69593 ]\nState: 15, action values:  [0. 0. 0. 0.]\n[[2 3 2 3]\n [2 0 2 0]\n [1 2 2 0]\n [0 1 1 0]]\nFalse\n"
    }
   ],
   "source": [
    "init_policy = np.ones((env.nS, env.nA), dtype=np.float32) / env.nA\n",
    "V_pi = policy_eavl(init_policy, env)\n",
    "improved_policy = policy_improve(V_pi, env)\n",
    "print(np.argmax(improved_policy, axis=1).reshape(env.desc.shape))\n",
    "print(np.argmax(init_policy) == np.argmax(improved_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, init_policy, discount= 1.0):\n",
    "    env = environment\n",
    "    \"\"\"\n",
    "    策略迭代\n",
    "        input: environment, init_policy, discount.\n",
    "        return: optimized policy for current issue.\n",
    "    \"\"\"\n",
    "    policy = init_policy.copy()\n",
    "    max_episode = 10000\n",
    "    for episode in range(max_episode):\n",
    "        \"策略评估\"\n",
    "        V_pi = policy_eavl(policy, env, discount)\n",
    "        old_action = np.argmax(policy, axis=1)\n",
    "        \"策略提高\"\n",
    "        policy= policy_improve(V_pi, env, discount)\n",
    "        optim_action = np.argmax(policy, axis=1)\n",
    "        print(\"Episode%d: \" % (episode))\n",
    "        print(optim_action.reshape(env.desc.shape),\"\\n\")\n",
    "\n",
    "        \"评判策略是否达到稳定\"\n",
    "        if (old_action == optim_action).all(): # 对np而言all表示与，any表示或\n",
    "            break\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0, delta: 1.86.\nepoch: 10, delta: 1.04.\nepoch: 20, delta: 1.00.\nepoch: 30, delta: 1.00.\nepoch: 40, delta: 1.00.\nFinal epoch: 49\nEpisode0: \n[[2 3 2 3]\n [2 0 2 0]\n [1 2 2 0]\n [0 1 1 0]] \n\nepoch: 0, delta: 2.00.\nepoch: 10, delta: 1.00.\nepoch: 20, delta: 1.00.\nepoch: 30, delta: 1.00.\nepoch: 40, delta: 1.00.\nFinal epoch: 49\nEpisode1: \n[[2 1 2 3]\n [2 0 2 0]\n [1 1 2 0]\n [0 1 1 0]] \n\nepoch: 0, delta: 2.00.\nepoch: 10, delta: 1.00.\nepoch: 20, delta: 1.00.\nepoch: 30, delta: 1.00.\nepoch: 40, delta: 1.00.\nFinal epoch: 49\nEpisode2: \n[[1 1 2 3]\n [2 0 2 0]\n [1 1 2 0]\n [0 1 1 0]] \n\nepoch: 0, delta: 2.00.\nepoch: 10, delta: 1.00.\nepoch: 20, delta: 1.00.\nepoch: 30, delta: 1.00.\nepoch: 40, delta: 1.00.\nFinal epoch: 49\nEpisode3: \n[[1 1 2 3]\n [2 0 2 0]\n [1 1 2 0]\n [0 1 1 0]] \n\nFinal policy: [[0. 1. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [1. 0. 0. 0.]\n [0. 0. 1. 0.]\n [1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 1. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 1. 0. 0.]\n [1. 0. 0. 0.]]\n"
    }
   ],
   "source": [
    "final_policy = policy_iteration(env, init_policy)\n",
    "print(\"Final policy:\", final_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38464bit48db41c0f43d45c882ae1cd0cd1553ec",
   "display_name": "Python 3.8.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}